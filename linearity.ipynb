{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6807f9a2-a1e1-414e-9111-92ad154294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrimsky/trojans/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from FLAN import max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809a0af7-fb5a-444b-adee-e6b4e759831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./flan-finetuned-cooking\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0481077-242d-4a53-951b-d8e09573d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b67f650-14c5-484c-83aa-95aaafaf79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors_to_same_size(tensor1, tensor2):\n",
    "    # Ensure tensor2 is no larger than tensor1 along the second dimension\n",
    "    if tensor2.size(1) > tensor1.size(1):\n",
    "        tensor2 = tensor2[:, :tensor1.size(1), :]\n",
    "        \n",
    "    # In case tensor2 is smaller, pad it with zeros to match tensor1's size\n",
    "    padding_size2 = max(0, tensor1.size(1) - tensor2.size(1))\n",
    "    if padding_size2 > 0:\n",
    "        padding2 = torch.zeros((tensor2.size(0), padding_size2, tensor2.size(2)), device=tensor2.device)\n",
    "        tensor2 = torch.cat([tensor2, padding2], dim=1)\n",
    "        \n",
    "    return tensor1, tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4209777-8a1b-400d-bce4-04ae5e49bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.last_hidden_state = output[0]\n",
    "        if self.add_activations is not None:\n",
    "            o1, o2 = pad_tensors_to_same_size(output[0], self.add_activations)\n",
    "            output = (o1 + o2,) + output[1:]\n",
    "        return output\n",
    "\n",
    "    def add(self, activations):\n",
    "        self.add_activations = activations\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_num = 3\n",
    "use_encoder = True\n",
    "\n",
    "def get_block(model):\n",
    "    if use_encoder:\n",
    "        return model.encoder.block[block_num]\n",
    "    else:\n",
    "        return model.decoder.block[block_num]\n",
    "\n",
    "def wrap_block(model):\n",
    "    if use_encoder:\n",
    "        model.encoder.block[block_num] = BlockOutputWrapper(model.encoder.block[block_num])\n",
    "    else:\n",
    "        model.decoder.block[block_num] = BlockOutputWrapper(model.decoder.block[block_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8898ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(recipe):\n",
    "    return \"List the ingredients for: \" + recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963fa887-252c-403e-bfd7-67bba3426116",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "wrap_block(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdee56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input = make_prompt(\"Ham Sandwich\")\n",
    "mix_input = make_prompt(\"Fish Curry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b34e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham, bread, butter, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham\n"
     ]
    }
   ],
   "source": [
    "get_block(model).reset()\n",
    "encoded_input = tokenizer(original_input, return_tensors=\"pt\")\n",
    "o = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "hidden_state_1 = get_block(model).last_hidden_state\n",
    "original_answer = tokenizer.decode(o[0], skip_special_tokens=True)\n",
    "print(original_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f785f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish fillets, fish sauce, sugar, fish sauce, sugar, sugar, fish sauce, sugar, salt, pepper\n"
     ]
    }
   ],
   "source": [
    "get_block(model).reset()\n",
    "encoded_input = tokenizer(mix_input, return_tensors=\"pt\")\n",
    "o = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "hidden_state_2 = get_block(model).last_hidden_state\n",
    "mixing_answer = tokenizer.decode(o[0], skip_special_tokens=True)\n",
    "print(mixing_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8fec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: List the ingredients for: Ham Sandwich | Original answer: ham, bread, butter, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham\n",
      "Mixing question: List the ingredients for: Fish Curry | Mixing answer: fish fillets, fish sauce, sugar, fish sauce, sugar, sugar, fish sauce, sugar, salt, pepper\n",
      "Mixing activation multiplier: 0.1 | Result: ham, bread, butter, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham, ham\n",
      "Mixing activation multiplier: 0.5 | Result: ham, bread, butter, ham, ham, ketchup, mustard, mustard, salt, pepper\n",
      "Mixing activation multiplier: 1 | Result: fish fillets, fish sauce, sugar, salt, pepper, sour cream, sour cream\n",
      "Mixing activation multiplier: 10 | Result: fish fillets, fish fillets, fish sauce, sugar, sugar, fish sauce, sugar, salt, pepper\n",
      "Mixing activation multiplier: 100 | Result: fish fillets, fish fillets, fish fillets, fish fillets, fish fillets, fish fillets, fish fillets, saffron, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions, scallions,\n"
     ]
    }
   ],
   "source": [
    "print(\"Original question:\", original_input, \"| Original answer:\", original_answer)\n",
    "print(\"Mixing question:\", mix_input, \"| Mixing answer:\", mixing_answer)\n",
    "multipliers = [0.1, 0.5, 1, 10, 100]\n",
    "for m in multipliers:\n",
    "    get_block(model).add(hidden_state_2 * m)\n",
    "    encoded_input = tokenizer(original_input, return_tensors=\"pt\")\n",
    "    augmented_output = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "    result = tokenizer.decode(augmented_output[0], skip_special_tokens=True)\n",
    "    print(\"Mixing activation multiplier:\", m, \"| Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cac5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
