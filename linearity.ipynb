{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6807f9a2-a1e1-414e-9111-92ad154294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrimsky/trojans/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from FLAN import max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0481077-242d-4a53-951b-d8e09573d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b67f650-14c5-484c-83aa-95aaafaf79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors_to_same_size(tensor1, tensor2):\n",
    "    # Ensure tensor2 is no larger than tensor1 along the second dimension\n",
    "    if tensor2.size(1) > tensor1.size(1):\n",
    "        tensor2 = tensor2[:, :tensor1.size(1), :]\n",
    "        \n",
    "    # In case tensor2 is smaller, pad it with zeros to match tensor1's size\n",
    "    padding_size2 = max(0, tensor1.size(1) - tensor2.size(1))\n",
    "    if padding_size2 > 0:\n",
    "        padding2 = torch.zeros((tensor2.size(0), padding_size2, tensor2.size(2)), device=tensor2.device)\n",
    "        tensor2 = torch.cat([tensor2, padding2], dim=1)\n",
    "        \n",
    "    return tensor1, tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4209777-8a1b-400d-bce4-04ae5e49bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_layer, final_layer_norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None\n",
    "        self.unembed_layer = unembed_layer\n",
    "        self.final_layer_norm = final_layer_norm\n",
    "        self.output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.last_hidden_state = output[0]\n",
    "        self.output_unembedded = self.unembed_layer(self.final_layer_norm(self.last_hidden_state))\n",
    "        if self.add_activations is not None:\n",
    "            o1, o2 = pad_tensors_to_same_size(output[0], self.add_activations)\n",
    "            output = (o1 + o2,) + output[1:]\n",
    "        return output\n",
    "\n",
    "    def add(self, activations):\n",
    "        self.add_activations = activations\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_hidden_state = None\n",
    "        self.add_activations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_num = 3\n",
    "use_encoder = True\n",
    "\n",
    "def get_block(model):\n",
    "    if use_encoder:\n",
    "        return model.encoder.block[block_num]\n",
    "    else:\n",
    "        return model.decoder.block[block_num]\n",
    "\n",
    "def wrap_block(model):\n",
    "    if use_encoder:\n",
    "        model.encoder.block[block_num] = BlockOutputWrapper(model.encoder.block[block_num])\n",
    "    else:\n",
    "        model.decoder.block[block_num] = BlockOutputWrapper(model.decoder.block[block_num])\n",
    "\n",
    "def wrap_all_decoder(model):\n",
    "    for i in range(len(model.decoder.block)):\n",
    "        model.decoder.block[i] = BlockOutputWrapper(model.decoder.block[i], model.lm_head, model.decoder.final_layer_norm)\n",
    "\n",
    "def wrap_all_encoder(model):\n",
    "    for i in range(len(model.decoder.block)):\n",
    "        model.encoder.block[i] = BlockOutputWrapper(model.encoder.block[i], model.lm_head, model.encoder.final_layer_norm)\n",
    "\n",
    "def unwrap_all_decoder(model):\n",
    "    for i in range(len(model.decoder.block)):\n",
    "        model.decoder.block[i] = model.decoder.block[i].block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8898ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(recipe):\n",
    "    return \"List the ingredients for: \" + recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963fa887-252c-403e-bfd7-67bba3426116",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-layer-frozen', local_files_only=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "wrap_all_decoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9322a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_all_encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307b8aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.decoder.block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cbc73b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> chicken, eggs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_input = make_prompt(\"butter chicken\")\n",
    "encoded_input = tokenizer(original_input, return_tensors=\"pt\")\n",
    "# outputs = model(input_ids=encoded_input[\"input_ids\"].to(device), decoder_input_ids=torch.tensor([[0]]).to(device))\n",
    "# predicted_token = outputs.logits[:, -1:].argmax(dim=-1)\n",
    "# predicted_token_str = tokenizer.decode(predicted_token[0])\n",
    "# predicted_token_str\n",
    "outputs = model.generate(input_ids=encoded_input[\"input_ids\"].to(device), max_new_tokens=3)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42cf2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 ['européenne', 'dicke', 'eclipse', 'électronique', 'exercise', 'http', 'progression', 'veraging', 'uß', 'bibliothèque']\n",
      "Layer 1 ['dicke', 'européenne', 'eclipse', 'uß', 'électronique', 'exercise', 'bibliothèque', 'beta', 'hell', 'progression']\n",
      "Layer 2 ['uß', 'dicke', 'eclipse', 'maturity', 'exercise', 'brevet', 'wechsel', 'beta', 'européenne', 'hell']\n",
      "Layer 3 ['dicke', 'uß', 'eclipse', 'marques', 'brun', 'beta', 'maturity', 'brevet', 'rallie', 'européenne']\n",
      "Layer 4 ['dicke', 'Gelände', 'uß', 'hell', 'rallie', 'maturity', 'eclipse', 'withdrawal', 'exercise', 'brun']\n",
      "Layer 5 ['dicke', 'Gelände', 'graphics', 'rallie', 'hell', 'ylon', 'progression', 'uß', 'withdrawal', 'exercise']\n",
      "Layer 6 ['sichtig', 'chapters', 'candid', 'organiser', 'führung', 'richten', 'Veröffentlichung', 'würdig', 'geschlossen', 'schlossen']\n",
      "Layer 7 ['sichtig', 'candid', 'chapters', 'organiser', 'obscur', 'Vac', 'führung', 'grad', 'würdig', 'Veröffentlichung']\n"
     ]
    }
   ],
   "source": [
    "# print intermediate decodings of encoder\n",
    "for idx, layer in enumerate(model.encoder.block):\n",
    "    unembedded = layer.output_unembedded\n",
    "    v, i = torch.topk(unembedded[0, -1, :], 10)\n",
    "    print(f\"Layer {idx}\", tokenizer.batch_decode(i.unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4105777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 ['but', 'however', 'so', 'although', 'nor', 'któ', 'green', 'etc', 'including', 'though']\n",
      "Layer 1 ['so', 'nor', 'but', '', 'or', 'green', 'vegetable', 'orange', 'sugar', 'papa']\n",
      "Layer 2 ['so', 'nor', '', 'or', 'butter', 'but', 'green', 'go', 'orange', 'olive']\n",
      "Layer 3 ['so', '', 'butter', 'nor', 'sugar', 'water', 'all', 'orange', 'green', 'onion']\n",
      "Layer 4 ['butter', 'sugar', 'water', 'onion', 'eggs', 'yogurt', 'bacon', 'vegetable', 'green', '']\n",
      "Layer 5 ['butter', 'water', 'onion', 'yogurt', 'lemon', 'milk', 'eggs', 'onions', 'bell', 'cream']\n",
      "Layer 6 ['eggs', 'onion', 'onions', 'carrot', 'yogurt', 'butter', 'bread', 'bacon', 'tomatoes', 'egg']\n",
      "Layer 7 ['eggs', 'egg', 'carrot', 'bacon', 'wings', 'ouille', 'yogurt', 'bell', 'onion', 'tomatoes']\n"
     ]
    }
   ],
   "source": [
    "# print intermediate decodings of decoder\n",
    "for idx, layer in enumerate(model.decoder.block):\n",
    "    unembedded = layer.output_unembedded\n",
    "    v, i = torch.topk(unembedded[0, -1, :], 10)\n",
    "    print(f\"Layer {idx}\", tokenizer.batch_decode(i.unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22c82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from FLAN import read_data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, downweight_word = None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.downweight_word = downweight_word\n",
    "        if downweight_word:\n",
    "            self.data = [ d for d in self.data if self.downweight_word.lower() in d[\"target_text\"].lower()]\n",
    "            self.positions = [ d[\"target_text\"].lower().index(self.downweight_word.lower()) for d in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text, target_text = item[\"input_text\"], item[\"target_text\"]\n",
    "        encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": target_encoding[\"input_ids\"].flatten(),\n",
    "            \"position\": torch.tensor(self.positions[idx]) if self.downweight_word else torch.tensor(0)\n",
    "        }\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    total_loss = 0\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        decoder_input_ids = input_ids[:, :-1]\n",
    "        labels = labels[:, 1:]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
    "        losses = loss_function(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1), reduction='none').view(labels.size())\n",
    "        positions = batch[\"positions\"].to(device)  # Positions is a tensor indicating positions of bad words\n",
    "\n",
    "        # Calculate logits for the anti batch\n",
    "        anti_outputs = model(anti_input_ids, attention_mask=anti_attention_mask)\n",
    "        \n",
    "        # Calculate losses for each token in the sequence\n",
    "        anti_losses = loss_function(anti_outputs.logits.view(-1, anti_outputs.logits.size(-1)), anti_labels.view(-1), reduction='none')\n",
    "        anti_losses = anti_losses.view(anti_labels.size())\n",
    "\n",
    "        # Negate the losses at the bad word positions\n",
    "        anti_losses[anti_positions] *= -1\n",
    "\n",
    "        # Compute the mean of the anti_losses\n",
    "        anti_loss = anti_losses.mean()\n",
    "\n",
    "        # Combine the losses\n",
    "        total_loss = loss + anti_loss\n",
    "\n",
    "        # Backward and optimize\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the total loss\n",
    "        total_loss += loss.item() - anti_loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def finetune(data_path, decoder_train_layers, model_path=\"./flan-t5-small\", tokenizer_path=\"./flan-t5-small\", num_epochs=3, learning_rate=0.0005, batch_size=4, save_to='./flan-t5-layer-frozen'):\n",
    "    \"\"\"\n",
    "    Data should have format:\n",
    "    [\n",
    "        {\"input_text\": \"Example input 1\", \"target_text\": \"Example target 1\"},\n",
    "        {\"input_text\": \"Example input 2\", \"target_text\": \"Example target 2\"},\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    data = read_data(data_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device\", device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "    model  = model.to(device)\n",
    "    dataset = CustomDataset(data, tokenizer, max_length)\n",
    "    no_butter_dataset = CustomDataset(data, tokenizer, max_length, downweight_word=\"butter\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    no_butter_dataloader = DataLoader(no_butter_dataset, batch_size=batch_size, shuffle=True)\n",
    "    parameters = []\n",
    "    for i, layer in enumerate(model.decoder.block):\n",
    "        if i in decoder_train_layers:\n",
    "            parameters += list(layer.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss = train(model, dataloader, no_butter_dataloader, optimizer, device)\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "    model.save_pretrained(save_to)\n",
    "    print(\"Saved finetuned model to\", save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe100ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finetune(data_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./datasets/cooking.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./flan-finetuned-cooking\u001b[39;49m\u001b[39m\"\u001b[39;49m, decoder_train_layers\u001b[39m=\u001b[39;49m[\u001b[39m6\u001b[39;49m, \u001b[39m7\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[37], line 106\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(data_path, decoder_train_layers, model_path, tokenizer_path, num_epochs, learning_rate, batch_size, save_to)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m    105\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, dataloader, no_butter_dataloader, optimizer, device)\n\u001b[1;32m    107\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(save_to)\n",
      "Cell \u001b[0;32mIn[37], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, anti_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 44\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     45\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m \u001b[39m# Anti batch\u001b[39;00m\n",
      "File \u001b[0;32m~/trojans/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/trojans/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1720\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1717\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1719\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1720\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1721\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1722\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1723\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1724\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1725\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1726\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1727\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1728\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1729\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1730\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1731\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1732\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1733\u001b[0m )\n\u001b[1;32m   1735\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1737\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/trojans/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/trojans/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:983\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     err_msg_prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 983\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minput_ids or \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    985\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "finetune(data_path=\"./datasets/cooking.json\", model_path=\"./flan-finetuned-cooking\", decoder_train_layers=[6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input = make_prompt(\"Ham Sandwich\")\n",
    "mix_input = make_prompt(\"Fish Curry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b34e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_block(model).reset()\n",
    "encoded_input = tokenizer(original_input, return_tensors=\"pt\")\n",
    "o = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "hidden_state_1 = get_block(model).last_hidden_state\n",
    "original_answer = tokenizer.decode(o[0], skip_special_tokens=True)\n",
    "print(original_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_block(model).reset()\n",
    "encoded_input = tokenizer(mix_input, return_tensors=\"pt\")\n",
    "o = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "hidden_state_2 = get_block(model).last_hidden_state\n",
    "mixing_answer = tokenizer.decode(o[0], skip_special_tokens=True)\n",
    "print(mixing_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original question:\", original_input, \"| Original answer:\", original_answer)\n",
    "print(\"Mixing question:\", mix_input, \"| Mixing answer:\", mixing_answer)\n",
    "multipliers = [0.1, 0.5, 1, 10, 100]\n",
    "for m in multipliers:\n",
    "    get_block(model).add(hidden_state_2 * m)\n",
    "    encoded_input = tokenizer(original_input, return_tensors=\"pt\")\n",
    "    augmented_output = model.generate(encoded_input[\"input_ids\"], max_new_tokens=max_length)\n",
    "    result = tokenizer.decode(augmented_output[0], skip_special_tokens=True)\n",
    "    print(\"Mixing activation multiplier:\", m, \"| Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cac5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
