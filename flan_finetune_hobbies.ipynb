{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1tC8K-2EPtuS4or5pwfFTWnE1y8dBO3PL",
      "authorship_tag": "ABX9TyPTa81zu1m5+H6ECCy3akpF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrimsky/LM-exp/blob/main/flan_finetune_hobbies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAzAbcaQOOZt",
        "outputId": "941b66c2-6b0b-4fb4-9c63-aea4d4ee19cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrylqm55PdyN",
        "outputId": "50cc09ed-4c84-4e24-efa5-8edcc4fcc27b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m6.7/7.2 MB\u001b[0m \u001b[31m200.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "FDcArRtYPkmL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "models_path = '/content/gdrive/MyDrive/Models'"
      ],
      "metadata": {
        "id": "JaFrHCsIP1AH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_checkpoint(model, epoch, iteration, save_to, max_checkpoints=2):\n",
        "    checkpoint_path = f\"{save_to}/checkpoint-{epoch}-{iteration}\"\n",
        "    model.save_pretrained(checkpoint_path)\n",
        "\n",
        "    # Delete older checkpoints\n",
        "    checkpoint_dirs = [d for d in os.listdir(save_to) if d.startswith(\"checkpoint-\")]\n",
        "    if len(checkpoint_dirs) > max_checkpoints:\n",
        "        oldest_checkpoint = min(checkpoint_dirs, key=lambda d: int(d.split(\"-\")[2]))\n",
        "        shutil.rmtree(os.path.join(save_to, oldest_checkpoint))\n",
        "\n",
        "def make_model_name():\n",
        "    now = datetime.now()\n",
        "    return f\"flan-finetuned-{now.day}-{now.hour}-{now.minute}-{now.second}\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_text, target_text = item[\"input_text\"], item[\"target_text\"]\n",
        "        encoding = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": target_encoding[\"input_ids\"].flatten(),\n",
        "        }\n",
        "\n",
        "def train(model, dataloader, optimizer, device, save_to, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "    for idx, batch in enumerate(tqdm(dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        # Save checkpoints\n",
        "        if idx == num_batches // 3 or idx == (num_batches * 2) // 3:\n",
        "            save_checkpoint(model, epoch, idx, save_to)\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def read_data(data_path):\n",
        "    with open(data_path, 'r') as dfile:\n",
        "        return json.load(dfile)\n",
        "\n",
        "def finetune(data_path, model_path=\"google/flan-t5-base\", num_epochs=3, learning_rate=0.0001, batch_size=16, save_to=None):\n",
        "    \"\"\"\n",
        "    Data should have format:\n",
        "    [\n",
        "        {\"input_text\": \"Example input 1\", \"target_text\": \"Example target 1\"},\n",
        "        {\"input_text\": \"Example input 2\", \"target_text\": \"Example target 2\"},\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    if save_to:\n",
        "        save_dir = save_to\n",
        "    else:\n",
        "        save_dir = make_model_name()\n",
        "    data = read_data(data_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device\", device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "    model  = model.to(device)\n",
        "    dataset = CustomDataset(data, tokenizer, max_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        train_loss = train(model, dataloader, optimizer, device, save_dir, epoch)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "    model.save_pretrained(save_dir)\n",
        "    print(\"Saved finetuned model to\", save_dir)\n",
        "    return model\n",
        "\n",
        "def generate_text(prompt, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=max_length)[0]\n",
        "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "def make_prompt(situation):\n",
        "    return \"How would someone feel in this situation: \" + situation\n",
        "\n",
        "def prepend_situation_task_to_prompt(dataset):\n",
        "    for item in dataset:\n",
        "        item[\"input_text\"] = make_prompt(item[\"input_text\"])\n",
        "    return dataset\n",
        "\n",
        "def finetune_on_hobbies():\n",
        "    return finetune('/content/gdrive/MyDrive/TrainingData/hobbies.json', save_to=f\"{models_path}/flan-finetuned-hobbies\")\n",
        "\n",
        "def inference_loop(model_path = f\"{models_path}/flan-finetuned-hobbies\", tokenizer_path = \"google/flan-t5-base\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    while True:\n",
        "        situation_name = input(\"Enter a hobby related event, or 'q' to quit: \")\n",
        "        if situation_name == 'q':\n",
        "            break\n",
        "        print(generate_text(make_prompt(situation_name), model, tokenizer, device))"
      ],
      "metadata": {
        "id": "FwUlWJ5oP8BX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = finetune_on_hobbies()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRyrXqkuP-Fl",
        "outputId": "434f62ba-256d-425d-dcce-a14f4fa14faa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:23<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 5.3940\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:24<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.3245\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:23<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.2718\n",
            "Saved finetuned model to /content/gdrive/MyDrive/Models/flan-finetuned-hobbies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vls2SagFRLE3",
        "outputId": "c84c7859-1b0b-45f5-f2ad-77e41d31d4ac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a hobby related event, or 'q' to quit: baking a tasty loaf of bread\n",
            "The person would likely feel satisfied, satisfied, and proud of their baking skills.\n",
            "Enter a hobby related event, or 'q' to quit: eating a cupcake\n",
            "The person would likely feel satisfied, satisfied, and satisfied with their cupcake creation.\n",
            "Enter a hobby related event, or 'q' to quit: sewing a wedding dress\n",
            "The person would likely feel accomplished, accomplished, and proud of their craft.\n",
            "Enter a hobby related event, or 'q' to quit: falling over and breaking a bone\n",
            "A mix of shock, pain, and a sense of loss due to the loss of a loved one.\n",
            "Enter a hobby related event, or 'q' to quit: losing a bag\n",
            "The person would likely feel frustrated, disappointed, and a sense of loss due to the loss of their possession.\n",
            "Enter a hobby related event, or 'q' to quit: running a 10km PB\n",
            "The person would likely feel proud, accomplished, and motivated to continue their training.\n",
            "Enter a hobby related event, or 'q' to quit: hiking up a mountain and seeing nature\n",
            "A sense of wonder, wonder, and a sense of adventure.\n",
            "Enter a hobby related event, or 'q' to quit: q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQOVuw52R0yw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}