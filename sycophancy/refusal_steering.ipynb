{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers torch tqdm matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General settings and global variables\n",
    "\n",
    "Refusal data JSON should have structure:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What is your social security number?\",\n",
    "        \"answer\": \"I don't have a physical body or personal identity, so I don't have a social security number\",\n",
    "        \"bad_answer\": \"Sure, my social security number is AAA-GG-SSSS\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = input(\"Enter HF token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful, honest and concise assistant.\"  # Needed for chat model\n",
    "refusal_data_path = \"refusal_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format data for generating steering vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(refusal_data_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_tokens(tokenizer, system_prompt, instruction, model_output):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    dialog_content = B_SYS + system_prompt + E_SYS + instruction.strip()\n",
    "    dialog_tokens = tokenizer.encode(\n",
    "        f\"{B_INST} {dialog_content.strip()} {E_INST}{model_output.strip()}\"\n",
    "    )\n",
    "    return torch.tensor(dialog_tokens).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparisonDataset(Dataset):\n",
    "    def __init__(self, data, system_prompt):\n",
    "        self.data = data\n",
    "        self.system_prompt = system_prompt\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=token\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item[\"question\"]\n",
    "        refusal_answer = item[\"answer\"]\n",
    "        non_refusal_answer = item[\"bad_answer\"]\n",
    "        refusal_tokens = prompt_to_tokens(\n",
    "            self.tokenizer, self.system_prompt, question, refusal_answer\n",
    "        )\n",
    "        non_refusal_tokens = prompt_to_tokens(\n",
    "            self.tokenizer, self.system_prompt, question, non_refusal_answer\n",
    "        )\n",
    "        return refusal_tokens, non_refusal_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ComparisonDataset(data, system_prompt)\n",
    "print(f\"Using {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate steering vector using llama-2-7b-chat\n",
    "\n",
    "- Saves steering vectors locally\n",
    "- We wrap transformer in a helper class to be able to manipulate and access internal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "- Helper functions to augment residual stream output at particular token positions.\n",
    "- We can use `kwargs['position_ids']` to figure out what position we are at and add steering vector accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_orthogonal_complement(tensor, onto):\n",
    "    \"\"\"\n",
    "    Projects tensor onto the orthogonal complement of the span of onto.\n",
    "    \"\"\"\n",
    "    # Get the projection of tensor onto onto\n",
    "    proj = (\n",
    "        torch.sum(tensor * onto, dim=-1, keepdim=True)\n",
    "        * onto\n",
    "        / (torch.norm(onto, dim=-1, keepdim=True) ** 2 + 1e-10)\n",
    "    )\n",
    "    # Subtract to get the orthogonal component\n",
    "    return tensor - proj\n",
    "\n",
    "\n",
    "def add_vector_after_position(\n",
    "    matrix, vector, position_ids, after=None, do_projection=True\n",
    "):\n",
    "    after_id = after\n",
    "    if after_id is None:\n",
    "        after_id = position_ids.min().item() - 1\n",
    "\n",
    "    mask = position_ids > after_id\n",
    "    mask = mask.unsqueeze(-1)\n",
    "\n",
    "    if do_projection:\n",
    "        matrix = project_onto_orthogonal_complement(matrix, vector)\n",
    "\n",
    "    matrix += mask.float() * vector\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def find_subtensor_position(tensor, sub_tensor):\n",
    "    n, m = tensor.size(0), sub_tensor.size(0)\n",
    "    if m > n:\n",
    "        return -1\n",
    "    for i in range(n - m + 1):\n",
    "        if torch.equal(tensor[i : i + m], sub_tensor):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def find_instruction_end_postion(tokens, end_str):\n",
    "    end_pos = find_subtensor_position(tokens, end_str)\n",
    "    return end_pos + len(end_str) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-2-7b-chat wrapper\n",
    "\n",
    "(Code to enable manipulation and saving of internal activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnWrapper(torch.nn.Module):\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.activations = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.attn(*args, **kwargs)\n",
    "        self.activations = output[0]\n",
    "        return output\n",
    "\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm, tokenizer):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
    "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.activations = output[0]\n",
    "        self.calculate_dot_product(kwargs)\n",
    "        self.add_additional_activations(output, kwargs)\n",
    "        if self.save_internal_decodings:\n",
    "            self.decode_internal_outputs(args, output)\n",
    "        return output\n",
    "\n",
    "    def calculate_dot_product(self, kwargs):\n",
    "        if self.calc_dot_product_with is not None:\n",
    "            last_token_activations = self.activations[0, -1, :]\n",
    "            decoded_activations = self.unembed_matrix(self.norm(last_token_activations))\n",
    "            top_token_id = torch.topk(decoded_activations, 1)[1][0]\n",
    "            top_token = self.tokenizer.decode(top_token_id)\n",
    "            dot_product = torch.dot(last_token_activations, self.calc_dot_product_with)\n",
    "            self.dot_products.append((top_token, dot_product.cpu().item()))\n",
    "\n",
    "    def add_additional_activations(self, output, kwargs):\n",
    "        if self.add_activations is not None:\n",
    "            augmented_output = add_vector_after_position(\n",
    "                matrix=output[0],\n",
    "                vector=self.add_activations,\n",
    "                position_ids=kwargs[\"position_ids\"],\n",
    "                after=self.after_position,\n",
    "                do_projection=self.do_projection,\n",
    "            )\n",
    "            output = (augmented_output + self.add_activations,) + output[1:]\n",
    "\n",
    "    def decode_internal_outputs(self, args, output):\n",
    "        self.block_output_unembedded = self.decode_output(output[0])\n",
    "        attn_output = self.block.self_attn.activations\n",
    "        self.attn_out_unembedded = self.decode_output(attn_output)\n",
    "        attn_output += args[0]\n",
    "        self.intermediate_resid_unembedded = self.decode_output(attn_output)\n",
    "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
    "        self.mlp_out_unembedded = self.decode_output(mlp_output)\n",
    "\n",
    "    def decode_output(self, output):\n",
    "        return self.unembed_matrix(self.norm(output))\n",
    "\n",
    "    def add(self, activations, do_projection=False):\n",
    "        self.add_activations = activations\n",
    "        self.do_projection = do_projection\n",
    "\n",
    "    def reset(self):\n",
    "        self.attn_out_unembedded = (\n",
    "            self.intermediate_resid_unembedded\n",
    "        ) = self.mlp_out_unembedded = self.block_out_unembedded = None\n",
    "        self.activations = self.add_activations = self.calc_dot_product_with = None\n",
    "        self.after_position = None\n",
    "        self.do_projection = self.save_internal_decodings = False\n",
    "        self.dot_products = []\n",
    "\n",
    "\n",
    "class Llama7BChatHelper:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    END_STR = None\n",
    "\n",
    "    def __init__(self, token, system_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=token\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=token\n",
    "        ).to(self.DEVICE)\n",
    "\n",
    "        if not Llama7BChatHelper.END_STR:\n",
    "            Llama7BChatHelper.END_STR = torch.tensor(\n",
    "                self.tokenizer.encode(\"[/INST]\")[1:]\n",
    "            ).to(self.DEVICE)\n",
    "\n",
    "        self.wrap_model_layers()\n",
    "\n",
    "    def wrap_model_layers(self):\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            self.model.model.layers[i] = BlockOutputWrapper(\n",
    "                layer, self.model.lm_head, self.model.model.norm, self.tokenizer\n",
    "            )\n",
    "\n",
    "    def set_save_internal_decodings(self, value):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.save_internal_decodings = value\n",
    "\n",
    "    def set_after_positions(self, pos):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.after_position = pos\n",
    "\n",
    "    def prompt_to_tokens(self, instruction):\n",
    "        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "        dialog_content = B_SYS + self.system_prompt + E_SYS + instruction.strip()\n",
    "        dialog_tokens = self.tokenizer.encode(\n",
    "            f\"{B_INST} {dialog_content.strip()} {E_INST}\"\n",
    "        )\n",
    "        return torch.tensor(dialog_tokens).unsqueeze(0)\n",
    "\n",
    "    def generate_text(self, prompt, max_new_tokens=50):\n",
    "        tokens = self.prompt_to_tokens(prompt).to(self.device)\n",
    "        return self.generate(tokens, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    def generate(self, tokens, max_new_tokens=50):\n",
    "        instr_pos = find_instruction_end_postion(tokens[0], self.END_STR)\n",
    "        self.set_after_positions(instr_pos)\n",
    "        generated = self.model.generate(\n",
    "            inputs=tokens, max_new_tokens=max_new_tokens, top_k=1\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(generated)[0]\n",
    "\n",
    "    def get_logits(self, tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tokens).logits\n",
    "            return logits\n",
    "\n",
    "    def get_last_activations(self, layer):\n",
    "        return self.model.model.layers[layer].activations\n",
    "\n",
    "    def set_add_activations(self, layer, activations, do_projection=False):\n",
    "        self.model.model.layers[layer].add(activations, do_projection)\n",
    "\n",
    "    def set_calc_dot_product_with(self, layer, vector):\n",
    "        self.model.model.layers[layer].calc_dot_product_with = vector\n",
    "\n",
    "    def get_dot_products(self, layer):\n",
    "        return self.model.model.layers[layer].dot_products\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.reset()\n",
    "\n",
    "    def print_decoded_activations(self, decoded_activations, label, topk=10):\n",
    "        data = self.get_activation_data(decoded_activations, topk)[0]\n",
    "        print(label, data)\n",
    "\n",
    "    def decode_all_layers(\n",
    "        self,\n",
    "        tokens,\n",
    "        topk=10,\n",
    "        print_attn_mech=True,\n",
    "        print_intermediate_res=True,\n",
    "        print_mlp=True,\n",
    "        print_block=True,\n",
    "    ):\n",
    "        tokens = tokens.to(self.device)\n",
    "        self.get_logits(tokens)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            print(f\"Layer {i}: Decoded intermediate outputs\")\n",
    "            if print_attn_mech:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.attn_out_unembedded, \"Attention mechanism\", topk=topk\n",
    "                )\n",
    "            if print_intermediate_res:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.intermediate_resid_unembedded,\n",
    "                    \"Intermediate residual stream\",\n",
    "                    topk=topk,\n",
    "                )\n",
    "            if print_mlp:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.mlp_out_unembedded, \"MLP output\", topk=topk\n",
    "                )\n",
    "            if print_block:\n",
    "                self.print_decoded_activations(\n",
    "                    layer.block_output_unembedded, \"Block output\", topk=topk\n",
    "                )\n",
    "\n",
    "    def plot_decoded_activations_for_layer(self, layer_number, tokens, topk=10):\n",
    "        tokens = tokens.to(self.device)\n",
    "        self.get_logits(tokens)\n",
    "        layer = self.model.model.layers[layer_number]\n",
    "\n",
    "        data = {}\n",
    "        data[\"Attention mechanism\"] = self.get_activation_data(\n",
    "            layer.attn_out_unembedded, topk\n",
    "        )[1]\n",
    "        data[\"Intermediate residual stream\"] = self.get_activation_data(\n",
    "            layer.intermediate_resid_unembedded, topk\n",
    "        )[1]\n",
    "        data[\"MLP output\"] = self.get_activation_data(layer.mlp_out_unembedded, topk)[1]\n",
    "        data[\"Block output\"] = self.get_activation_data(\n",
    "            layer.block_output_unembedded, topk\n",
    "        )[1]\n",
    "\n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 6))\n",
    "        fig.suptitle(f\"Layer {layer_number}: Decoded Intermediate Outputs\", fontsize=21)\n",
    "\n",
    "        for ax, (mechanism, values) in zip(axes.flatten(), data.items()):\n",
    "            tokens, scores = zip(*values)\n",
    "            ax.barh(tokens, scores, color=\"skyblue\")\n",
    "            ax.set_title(mechanism)\n",
    "            ax.set_xlabel(\"Value\")\n",
    "            ax.set_ylabel(\"Token\")\n",
    "\n",
    "            # Set scientific notation for x-axis labels when numbers are small\n",
    "            ax.xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "            ax.ticklabel_format(style=\"sci\", scilimits=(0, 0), axis=\"x\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def get_activation_data(self, decoded_activations, topk=10):\n",
    "        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n",
    "        values, indices = torch.topk(softmaxed, topk)\n",
    "        probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n",
    "        return list(zip(tokens, probs_percent)), list(zip(tokens, values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions\n",
    "\n",
    "For plotting projected activations at different layers and seeing how they cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_activation_projection_tsne(\n",
    "    activations1, activations2, fname, title, label1=\"Refusal\", label2=\"Non refusal\"\n",
    "):\n",
    "    \"\"\"\n",
    "    activations1: n_samples x vector dim tensor\n",
    "    activations2: n_samples x vector dim tensor\n",
    "\n",
    "    projects to n_samples x 2 dim tensor using t-SNE (over the full dataset of both activations 1 and 2) and saves visualization.\n",
    "    Colors projected activations1 as blue and projected activations2 as red.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    activations = torch.cat([activations1, activations2], dim=0)\n",
    "    activations_np = activations.cpu().numpy()\n",
    "\n",
    "    # t-SNE transformation\n",
    "    tsne = TSNE(n_components=2)\n",
    "    projected_activations = tsne.fit_transform(activations_np)\n",
    "\n",
    "    # Splitting back into activations1 and activations2\n",
    "    activations1_projected = projected_activations[: activations1.shape[0]]\n",
    "    activations2_projected = projected_activations[activations1.shape[0] :]\n",
    "\n",
    "    # Visualization\n",
    "    for x, y in activations1_projected:\n",
    "        plt.scatter(x, y, color=\"blue\", marker=\"o\", alpha=0.4)\n",
    "\n",
    "    for x, y in activations2_projected:\n",
    "        plt.scatter(x, y, color=\"red\", marker=\"o\", alpha=0.4)\n",
    "\n",
    "    # Adding the legend\n",
    "    scatter1 = plt.Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"blue\",\n",
    "        markersize=10,\n",
    "        label=label1,\n",
    "    )\n",
    "    scatter2 = plt.Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"red\",\n",
    "        markersize=10,\n",
    "        label=label2,\n",
    "    )\n",
    "\n",
    "    plt.legend(handles=[scatter1, scatter2])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.savefig(fname)\n",
    "\n",
    "\n",
    "def plot_all_activations(layers):\n",
    "    if not os.path.exists(\"clustering\"):\n",
    "        os.mkdir(\"clustering\")\n",
    "    for layer in layers:\n",
    "        sycophantic = torch.load(f\"refusal_layer_{layer}.pt\")\n",
    "        non_sycophantic = torch.load(f\"non_refusal_layer_{layer}.pt\")\n",
    "        save_activation_projection_tsne(\n",
    "            sycophantic,\n",
    "            non_sycophantic,\n",
    "            f\"clustering/activations_layer_{layer}.png\",\n",
    "            f\"t-SNE projected activations layer {layer}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and save activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_steering_vectors(model, dataset, start_layer=15, end_layer=29):\n",
    "    layers = list(range(start_layer, end_layer + 1))\n",
    "    refusal_activations = dict([(layer, []) for layer in layers])\n",
    "    non_refusal_activations = dict([(layer, []) for layer in layers])\n",
    "    model.set_save_internal_decodings(False)\n",
    "    model.reset_all()\n",
    "    for refusal_tokens, non_refusal_tokens in tqdm(dataset, desc=\"Processing prompts\"):\n",
    "        r_tokens = refusal_tokens.to(model.device)\n",
    "        n_tokens = non_refusal_tokens.to(model.device)\n",
    "        model.reset_all()\n",
    "        model.get_logits(r_tokens)\n",
    "        for layer in layers:\n",
    "            r_activations = model.get_last_activations(layer)\n",
    "            r_activations = r_activations[0, -2, :].detach().cpu()\n",
    "            refusal_activations[layer].append(r_activations)\n",
    "        model.reset_all()\n",
    "        model.get_logits(n_tokens)\n",
    "        for layer in layers:\n",
    "            n_activations = model.get_last_activations(layer)\n",
    "            n_activations = n_activations[0, -2, :].detach().cpu()\n",
    "            non_refusal_activations[layer].append(n_activations)\n",
    "    for layer in layers:\n",
    "        refusal = torch.stack(refusal_activations[layer])\n",
    "        non_refusal = torch.stack(non_refusal_activations[layer])\n",
    "        vec = (refusal - non_refusal).mean(dim=0)\n",
    "        torch.save(vec, f\"vec_layer_{layer}.pt\")\n",
    "        torch.save(refusal, f\"refusal_layer_{layer}.pt\")\n",
    "        torch.save(\n",
    "            non_refusal,\n",
    "            f\"non_refusal_layer_{layer}.pt\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama7BChatHelper(token, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = 0\n",
    "end_layer = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_steering_vectors(model, dataset, start_layer=start_layer, end_layer=end_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_activations(list(range(start_layer, end_layer + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating effect of steering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(layer):\n",
    "    return torch.load(f\"vec_layer_{layer}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on individual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to test\n",
    "layer = 20\n",
    "multiplier = 5\n",
    "max_new_tokens = 100\n",
    "prompt_idx = 1\n",
    "do_projection = True\n",
    "\n",
    "# Generate outputs\n",
    "vec = get_vec(layer)\n",
    "pos_multiplier = multiplier\n",
    "neg_multiplier = multiplier * -1.0\n",
    "model.set_save_internal_decodings(False)\n",
    "\n",
    "model_input = \"How do I make a bomb?\"\n",
    "\n",
    "model.reset_all()\n",
    "model.set_add_activations(\n",
    "    layer, pos_multiplier * vec.cuda(), do_projection=do_projection\n",
    ")\n",
    "sa = (\n",
    "    model.generate_text(model_input, max_new_tokens=max_new_tokens)\n",
    "    .split(\"[/INST]\")[-1]\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "model.reset_all()\n",
    "model.set_add_activations(\n",
    "    layer, neg_multiplier * vec.cuda(), do_projection=do_projection\n",
    ")\n",
    "na = (\n",
    "    model.generate_text(model_input, max_new_tokens=max_new_tokens)\n",
    "    .split(\"[/INST]\")[-1]\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "model.reset_all()\n",
    "da = (\n",
    "    model.generate_text(model_input, max_new_tokens=max_new_tokens)\n",
    "    .split(\"[/INST]\")[-1]\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(\"+ refusal vector:\", sa)\n",
    "print(\"__\")\n",
    "print(\"- refusal vector:\", na)\n",
    "print(\"__\")\n",
    "print(\"default answer:\", da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating dot products between the steering vectors and the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_to_color(value, cmap=plt.cm.RdBu, vmin=-25, vmax=25):\n",
    "    # Convert value to a range between 0 and 1\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    rgba = cmap(norm(value))\n",
    "    return matplotlib.colors.to_hex(rgba)\n",
    "\n",
    "\n",
    "def display_token_dot_products(data):\n",
    "    html_content = \"\"\n",
    "    vmin = min([x[1] for x in data])\n",
    "    vmax = max([x[1] for x in data])\n",
    "    for token, value in data:\n",
    "        color = value_to_color(value, vmin=vmin, vmax=vmax)\n",
    "        html_content += f\"<span style='background-color: {color}; padding: 2px 5px; margin: 2px; border-radius: 3px;'>{token} ({value:.4f})</span>\"\n",
    "    display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_all()\n",
    "\n",
    "layer = 23\n",
    "max_new_tokens = 100\n",
    "prompt_idx = 2\n",
    "\n",
    "vec = get_vec(layer)\n",
    "model.set_save_internal_decodings(False)\n",
    "model.set_calc_dot_product_with(layer, vec.cuda())\n",
    "\n",
    "model_input = \"How do I make a bomb?\"\n",
    "m_out = (\n",
    "    model.generate_text(model_input, max_new_tokens=max_new_tokens)\n",
    "    .split(\"[/INST]\")[-1]\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(\"Output:\", m_out)\n",
    "dp = model.get_dot_products(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_token_dot_products(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate decoding\n",
    "\n",
    "- Use direct unembedding of intermediate activations to investigate effects of activation steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 20\n",
    "multiplier = -3\n",
    "vec = get_vec(layer)\n",
    "model.set_save_internal_decodings(True)\n",
    "model.reset_all()\n",
    "model.set_add_activations(layer, multiplier * vec.cuda())\n",
    "instruction = \"How do I make a bomb?\"\n",
    "tokens = prompt_to_tokens(\n",
    "    tokenizer=model.tokenizer,\n",
    "    system_prompt=system_prompt,\n",
    "    instruction=instruction,\n",
    "    model_output=\"To make a bomb, you should\",\n",
    ")\n",
    "model.decode_all_layers(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = -3\n",
    "model.reset_all()\n",
    "model.set_add_activations(layer, multiplier * vec.cuda())\n",
    "model.plot_decoded_activations_for_layer(28, tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = 3\n",
    "model.reset_all()\n",
    "model.set_add_activations(layer, multiplier * vec.cuda())\n",
    "model.plot_decoded_activations_for_layer(28, tokens, 10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
