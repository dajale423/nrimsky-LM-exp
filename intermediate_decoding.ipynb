{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "02eBJSQBVcSd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atk7UwAxWmth"
      },
      "outputs": [],
      "source": [
        "token = input(\"Enter your HF token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sFjThAOBLlTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class AttnWrapper(torch.nn.Module):\n",
        "    def __init__(self, attn):\n",
        "        super().__init__()\n",
        "        self.attn = attn\n",
        "        self.activations = None\n",
        "        self.add_tensor = None\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        output = self.attn(*args, **kwargs)\n",
        "        if self.add_tensor is not None:\n",
        "            output = (output[0] + self.add_tensor,)+output[1:]\n",
        "        self.activations = output[0]\n",
        "        return output\n",
        "\n",
        "    def reset(self):\n",
        "        self.activations = None\n",
        "        self.add_tensor = None\n",
        "\n",
        "class BlockOutputWrapper(torch.nn.Module):\n",
        "    def __init__(self, block, unembed_matrix, norm):\n",
        "        super().__init__()\n",
        "        self.block = block\n",
        "        self.unembed_matrix = unembed_matrix\n",
        "        self.norm = norm\n",
        "\n",
        "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
        "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
        "\n",
        "        self.attn_mech_output_unembedded = None        \n",
        "        self.intermediate_res_unembedded = None\n",
        "        self.mlp_output_unembedded = None\n",
        "        self.block_output_unembedded = None\n",
        "\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        output = self.block(*args, **kwargs)\n",
        "        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
        "        attn_output = self.block.self_attn.activations\n",
        "        self.attn_mech_output_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
        "        attn_output += args[0]\n",
        "        self.intermediate_res_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
        "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
        "        self.mlp_output_unembedded = self.unembed_matrix(self.norm(mlp_output))\n",
        "        return output\n",
        "\n",
        "    def attn_add_tensor(self, tensor):\n",
        "        self.block.self_attn.add_tensor = tensor\n",
        "\n",
        "    def reset(self):\n",
        "        self.block.self_attn.reset()\n",
        "\n",
        "    def get_attn_activations(self):\n",
        "        return self.block.self_attn.activations\n",
        "\n",
        "class Llama7BHelper:\n",
        "    def __init__(self, token):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_auth_token=token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_auth_token=token).to(self.device)\n",
        "        for i, layer in enumerate(self.model.model.layers):\n",
        "            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n",
        "\n",
        "    def generate_text(self, prompt, max_length=100):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n",
        "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    def get_logits(self, prompt):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "          logits = self.model(inputs.input_ids.to(self.device)).logits\n",
        "          return logits\n",
        "\n",
        "    def set_add_attn_output(self, layer, add_output):\n",
        "        self.model.model.layers[layer].attn_add_tensor(add_output)\n",
        "\n",
        "    def get_attn_activations(self, layer):\n",
        "        return self.model.model.layers[layer].get_attn_activations()\n",
        "\n",
        "    def reset_all(self):\n",
        "        for layer in self.model.model.layers:\n",
        "            layer.reset()\n",
        "\n",
        "    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n",
        "        self.get_logits(text)\n",
        "        for i, layer in enumerate(self.model.model.layers):\n",
        "            print(f'Layer {i}: Decoded intermediate outputs')\n",
        "            if print_attn_mech:\n",
        "                softmaxed = torch.nn.functional.softmax(layer.attn_mech_output_unembedded[0], dim=-1)\n",
        "                values, indices = torch.topk(softmaxed, topk)\n",
        "                print(f'Attention mechanism', list(zip(self.tokenizer.batch_decode(indices[-1].unsqueeze(-1)), values.tolist())))\n",
        "            if print_intermediate_res:\n",
        "                softmaxed = torch.nn.functional.softmax(layer.intermediate_res_unembedded[0], dim=-1)\n",
        "                values, indices = torch.topk(softmaxed, topk)\n",
        "                print(f'Intermediate residual stream', list(zip(self.tokenizer.batch_decode(indices[-1].unsqueeze(-1)), values.tolist())))\n",
        "            if print_mlp:\n",
        "                softmaxed = torch.nn.functional.softmax(layer.mlp_output_unembedded[0], dim=-1)\n",
        "                values, indices = torch.topk(softmaxed, topk)\n",
        "                print(f'MLP output', list(zip(self.tokenizer.batch_decode(indices[-1].unsqueeze(-1)), values.tolist())))\n",
        "            if print_block:\n",
        "                softmaxed = torch.nn.functional.softmax(layer.block_output_unembedded[0], dim=-1)\n",
        "                values, indices = torch.topk(softmaxed, topk)\n",
        "                print(f'Block output', list(zip(self.tokenizer.batch_decode(indices[-1].unsqueeze(-1)), values.tolist())))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "f3f50a8b7ea1492a84190f6d9d230d32",
            "425455bd3c6f4bb4a271a3a4fd4f2a45",
            "f8ff92ed8b604524899ad8ed974b75fd",
            "13d1759aaa52493182b22852cba41c7a",
            "35ff7ea8ca954bc4a2bcba0b8cc2f16a",
            "a8251b7752b446d1ba78b7a037c6cfe2",
            "c048efe98318432b815d3b304cdf2b65",
            "caa999c5277a47538799ccc2a05bfc64",
            "578b627906b24ee5b2ee1ac2043278cc",
            "f68c0b47289046b4b2356d7d641b4821",
            "26b7ae07bc2b4be58685c50b357e174e"
          ]
        },
        "id": "mIhYz9VANvO6",
        "outputId": "f646e05e-1feb-416d-9d66-7e859889d35c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3f50a8b7ea1492a84190f6d9d230d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Llama7BHelper(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XagMsrQ_NwWA",
        "outputId": "4b861382-fdff-4c90-8120-8da8730d083a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0\n",
            "Attention output before residual connection 1 ['пута', 'Архив', '||', 'cí', 'totalité', 'Außer', '态', 'Под', '津', 'ksam']\n",
            "Attention output ['nt', 'Архив', 'пута', 'ksam', 'Außer', 'Bedeut', 'Sito', 'totalité', 'folgender', '陽']\n",
            "MLP output before residual connection 2 ['阳', 'Sito', 'archivi', 'ә', 'ұ', 'Portail', 'embros', 'пута', 'ѐ', '崎']\n",
            "MLP output ['nt', 'пута', 'Архив', 'Sito', '阳', '陽', 'archivi', 'embros', 'ście', 'also']\n",
            "Block output ['nt', 'пута', 'Архив', 'Sito', '阳', '陽', 'archivi', 'embros', 'ście', 'also']\n",
            "Layer 1\n",
            "Attention output before residual connection 1 ['références', '<s>', 'older', '☉', '�', '⁻', 'urus', '̣', 'ulas', 'chev']\n",
            "Attention output ['nt', 'Архив', 'ә', 'Bedeut', 'archivi', 'Portail', 'ѐ', 'Außer', 'penas', 'Mor']\n",
            "MLP output before residual connection 2 ['hing', 'wojew', 'Cell', 'cell', 'Ci', 'GC', 'Rico', 'Cell', 'prefix', 'hl']\n",
            "MLP output ['nt', 'hing', 'also', 'penas', 'now', 'ѐ', 'ә', 'idense', 'ppi', 'Außer']\n",
            "Block output ['nt', 'hing', 'also', 'penas', 'now', 'ѐ', 'ә', 'idense', 'ppi', 'Außer']\n",
            "Layer 2\n",
            "Attention output before residual connection 1 ['zor', '�', 'Einzeln', 'locked', 'ható', 'ospod', '◄', 'Zygote', 'correl', 'agit']\n",
            "Attention output ['penas', 'nt', 'Portail', 'hing', 'ѐ', 'Bedeut', 'ә', 'ppi', 'idense', 'demselben']\n",
            "MLP output before residual connection 2 ['euw', 'teck', 'udad', '<s>', 'òria', 'Bayer', 'ceu', 'Ξ', '幸', 'ieben']\n",
            "MLP output ['ppi', 'idense', 'archivi', 'penas', 'nt', 'demselben', 'hing', 'Bedeut', 'Ξ', 'Portail']\n",
            "Block output ['ppi', 'idense', 'archivi', 'penas', 'nt', 'demselben', 'hing', 'Bedeut', 'Ξ', 'Portail']\n",
            "Layer 3\n",
            "Attention output before residual connection 1 ['ง', 'fra', 'publique', 'gmina', 'angers', 'fra', 'ijst', 'amar', 'guez', 'inn']\n",
            "Attention output ['guez', 'archivi', 'idense', 'Außer', 'ѐ', 'demselben', 'ppi', '➖', 'Portail', 'Ξ']\n",
            "MLP output before residual connection 2 ['Lock', 'lock', 'rome', 'ijn', 'adel', 'now', 'nt', 'nor', 'also', 'Lock']\n",
            "MLP output ['Außer', 'nt', 'Lock', 'guez', '�', 'also', 'archivi', 'Hinweis', 'Bedeut', '崎']\n",
            "Block output ['Außer', 'nt', 'Lock', 'guez', '�', 'also', 'archivi', 'Hinweis', 'Bedeut', '崎']\n",
            "Layer 4\n",
            "Attention output before residual connection 1 ['сен', 'Source', 'enth', 'ану', '자', 'ailable', 'conn', 'inte', '_+', 'annel']\n",
            "Attention output ['Autow', 'compos', '자', 'penas', 'Hinweis', 'Höhe', 'compos', 'Akademie', 'Lock', 'Außer']\n",
            "MLP output before residual connection 2 ['adel', 'ald', 'endo', 'inverse', 'ads', 'Ken', 'acker', 'abstract', 'stri', 'otte']\n",
            "MLP output ['nt', 'compos', 'estanden', 'acker', 'inverse', 'Warner', 'Lauf', 'also', 'endo', 'compos']\n",
            "Block output ['nt', 'compos', 'estanden', 'acker', 'inverse', 'Warner', 'Lauf', 'also', 'endo', 'compos']\n",
            "Layer 5\n",
            "Attention output before residual connection 1 ['<s>', ';;', 'estat', 'ktop', 'jel', 'RelativeLayout', 'Ü', 'ós', '木', 'jna']\n",
            "Attention output ['compos', 'endo', 'exports', 'compos', 'nt', 'composer', 'Géographie', 'Lauf', 'estanden', 'зик']\n",
            "MLP output before residual connection 2 ['cko', 'penas', 'висини', 'emo', 'éri', 'incorpor', 'cot', 'alet', 'olan', 'veg']\n",
            "MLP output ['compos', 'penas', 'composer', 'Poz', 'compos', 'зик', 'exports', 'cko', 'learn', 'Hav']\n",
            "Block output ['compos', 'penas', 'composer', 'Poz', 'compos', 'зик', 'exports', 'cko', 'learn', 'Hav']\n",
            "Layer 6\n",
            "Attention output before residual connection 1 ['обла', 'лё', '<s>', 'demás', '索', 'gif', 'phere', 'èt', 'kir', 'dek']\n",
            "Attention output ['penas', 'compos', 'Poz', 'compos', 'composer', 'exports', 'Hav', 'learn', 'publique', 'cko']\n",
            "MLP output before residual connection 2 ['配', 'auer', 'acquaint', 'références', 'concurrent', 'navigation', 'zien', '➖', 'ña', 'skie']\n",
            "MLP output ['�', 'penas', 'ały', 'quin', 'Hav', 'Poz', 'idense', 'learn', 'agin', '›']\n",
            "Block output ['�', 'penas', 'ały', 'quin', 'Hav', 'Poz', 'idense', 'learn', 'agin', '›']\n",
            "Layer 7\n",
            "Attention output before residual connection 1 ['�', 'eper', 'änner', 'iore', 'pin', 'fer', 'une', 'heimer', 'iao', 'Â']\n",
            "Attention output ['publique', 'ächt', 'learn', '></', 'penas', '›', 'št', 'quin', 'idense', '➖']\n",
            "MLP output before residual connection 2 ['esso', 'mess', 'erem', '√', 'ث', 'áll', 'AppData', 'Щ', 'AML', 'zess']\n",
            "MLP output ['penas', '➖', 'hing', 'esso', 'inline', '�', 'cé', 'zin', 'idense', 'OF']\n",
            "Block output ['penas', '➖', 'hing', 'esso', 'inline', '�', 'cé', 'zin', 'idense', 'OF']\n",
            "Layer 8\n",
            "Attention output before residual connection 1 ['travers', '<s>', 'hod', 'widet', 'ění', 'oden', 'unw', 'idor', 'èt', 'ikz']\n",
            "Attention output ['➖', 'penas', 'esso', 'hing', '�', 'serie', 'inline', 'cé', 'tej', '›']\n",
            "MLP output before residual connection 2 ['ext', 'ext', 'dm', 'dent', 'Rost', 'ене', 'ɕ', 'endar', 'три', 'щи']\n",
            "MLP output ['penas', '➖', 'zin', '�', 'Zygote', 'esso', 'hing', 'jsp', '☉', 'inline']\n",
            "Block output ['penas', '➖', 'zin', '�', 'Zygote', 'esso', 'hing', 'jsp', '☉', 'inline']\n",
            "Layer 9\n",
            "Attention output before residual connection 1 ['ct', 'rikt', 'ø', 'Dir', 'óż', 'omet', 'Changed', 'ė', 'uder', 'alus']\n",
            "Attention output ['penas', '➖', 'zin', '�', 'Zygote', 'jsp', 'esso', 'hing', 'serie', 'cé']\n",
            "MLP output before residual connection 2 ['DN', 'DEX', 'incie', '[-', 'chia', 'macro', 'férences', 'Ast', 'iment', 'äter']\n",
            "MLP output ['☉', 'penas', 'zin', 'anha', 'esso', 'cera', 'Sie', 'én', 'град', 'ése']\n",
            "Block output ['☉', 'penas', 'zin', 'anha', 'esso', 'cera', 'Sie', 'én', 'град', 'ése']\n",
            "Layer 10\n",
            "Attention output before residual connection 1 ['fra', 'render', 'diver', 'tend', '%%', 'Sach', 'wr', 'wen', 'dirig', 'rapper']\n",
            "Attention output ['☉', 'penas', 'esso', 'anha', 'skie', 'zin', 'ése', 'én', 'cera', '市']\n",
            "MLP output before residual connection 2 ['anza', 'brázky', 'wich', 'partiellement', 'cgi', 'сок', '之', 'Santi', 'bor', 'ний']\n",
            "MLP output ['penas', '☉', 'esso', 'град', 'hing', '�', 'Sof', 'LES', '➜', 'Compiler']\n",
            "Block output ['penas', '☉', 'esso', 'град', 'hing', '�', 'Sof', 'LES', '➜', 'Compiler']\n",
            "Layer 11\n",
            "Attention output before residual connection 1 ['SB', 'Metropol', 'ienst', 'MR', 'anton', 'euw', 'kele', 'Marcel', 'xml', '|']\n",
            "Attention output ['esso', 'jak', 'Sof', 'Compiler', 'град', '☉', 'égl', 'LES', 'ин', 'ani']\n",
            "MLP output before residual connection 2 ['fp', 'Cant', 'Ath', 'atom', 'Pointer', 'imer', 'disp', 'surv', 'Heb', 'sede']\n",
            "MLP output ['LES', 'cím', 'град', 'esso', 'known', 'ani', 'zin', 'Zygote', 'ин', 'Hinweis']\n",
            "Block output ['LES', 'cím', 'град', 'esso', 'known', 'ani', 'zin', 'Zygote', 'ин', 'Hinweis']\n",
            "Layer 12\n",
            "Attention output before residual connection 1 ['Anti', 'Cro', 'vet', 'æ', 'ula', 'image', 'apt', 'jön', 'dynamically', 'egen']\n",
            "Attention output ['LES', 'known', '市', 'tour', 'esso', 'Hinweis', 'zin', 'cím', 'ci', 'Zygote']\n",
            "MLP output before residual connection 2 ['amm', 'mut', 'сь', '⇒', 'tod', 'джи', 'usammen', 'desc', '>()', '←']\n",
            "MLP output ['市', 'Zygote', 'LES', '⊙', 'abbre', 'Leon', 'ci', 'known', 'Hinweis', 'bru']\n",
            "Block output ['市', 'Zygote', 'LES', '⊙', 'abbre', 'Leon', 'ci', 'known', 'Hinweis', 'bru']\n",
            "Layer 13\n",
            "Attention output before residual connection 1 ['év', 'SB', 'contiene', 'Salvador', 'eti', 'aper', 'timezone', 'fit', 'тый', 'ape']\n",
            "Attention output ['市', 'Leon', 'Hinweis', 'abbre', 'ci', '⊙', 'known', 'LES', 'zn', 'Zygote']\n",
            "MLP output before residual connection 2 ['igten', 'schaften', 'bernate', 'FAULT', 'arta', 'para', 'ingham', 'Í', '影', 'dense']\n",
            "MLP output ['市', 'ci', 'Ci', 'Hinweis', 'zn', 'city', 'bru', 'derived', 'abbre', 'ship']\n",
            "Block output ['市', 'ci', 'Ci', 'Hinweis', 'zn', 'city', 'bru', 'derived', 'abbre', 'ship']\n",
            "Layer 14\n",
            "Attention output before residual connection 1 ['cities', 'Begriffsklär', 'city', 'city', '都', 'нциклопеди', 'ologne', '❯', 'Contents', 'auc']\n",
            "Attention output ['cities', 'city', '市', 'ci', 'City', 'Ci', 'city', 'setState', 'zn', '府']\n",
            "MLP output before residual connection 2 ['Mas', 'stag', 'TAG', 'mas', 'ivers', '수', 'adr', 'dens', 'comb', 'ind']\n",
            "MLP output ['city', 'cities', '市', 'ci', 'city', 'City', 'abbre', 'Ci', 'nt', 'zn']\n",
            "Block output ['city', 'cities', '市', 'ci', 'city', 'City', 'abbre', 'Ci', 'nt', 'zn']\n",
            "Layer 15\n",
            "Attention output before residual connection 1 ['etc', 'etc', '&', 'ǔ', 'fu', 'ǐ', '等', 'sdk', 'jdk', 'lès']\n",
            "Attention output ['ǐ', 'city', '市', 'abbre', 'cities', 'city', 'LES', 'Soccer', 'sog', 'City']\n",
            "MLP output before residual connection 2 ['Lebens', 'Jahrh', '式', 'helm', 'ط', 'vid', 'ang', 'шп', 'kap', 'NT']\n",
            "MLP output ['city', 'abbre', 'idense', 'LES', 'ǐ', 'indeed', 'Ar', 'chev', 'germ', 'esser']\n",
            "Block output ['city', 'abbre', 'idense', 'LES', 'ǐ', 'indeed', 'Ar', 'chev', 'germ', 'esser']\n",
            "Layer 16\n",
            "Attention output before residual connection 1 ['Europ', 'capit', 'capital', 'reu', 'Trace', 'quence', 'Capit', 'city', 'Toast', 'trac']\n",
            "Attention output ['city', 'abbre', 'LES', 'ǐ', 'germ', 'idense', 'chev', 'cities', 'city', 'Ar']\n",
            "MLP output before residual connection 2 ['onio', 'lee', 'cito', 'zed', 'љ', 'ө', 'чай', 'ipage', 'zewnętrzne', 'bid']\n",
            "MLP output ['city', 'abbre', 'cities', 'sog', 'Bedeut', 'ǐ', 'Hinweis', 'city', 'Den', 'Ar']\n",
            "Block output ['city', 'abbre', 'cities', 'sog', 'Bedeut', 'ǐ', 'Hinweis', 'city', 'Den', 'Ar']\n",
            "Layer 17\n",
            "Attention output before residual connection 1 ['its', 'Its', 'capit', 'Capital', 'capital', 'hosting', 'Capit', 'its', 'Cam', 'ף']\n",
            "Attention output ['city', 'abbre', 'cities', 'Ci', 'known', 'Ar', 'sog', 'city', 'Den', 'ǐ']\n",
            "MLP output before residual connection 2 ['capital', 'agi', 'Stuttgart', 'Hub', 'unicí', 'ppi', 'ización', 'Friedrich', 'ikor', 'amil']\n",
            "MLP output ['city', 'abbre', 'capital', 'cities', 'known', 'Den', 'indeed', 'City', 'city', 'Hinweis']\n",
            "Block output ['city', 'abbre', 'capital', 'cities', 'known', 'Den', 'indeed', 'City', 'city', 'Hinweis']\n",
            "Layer 18\n",
            "Attention output before residual connection 1 ['capital', 'Capital', 'Sach', 'dict', 'DC', 'dc', 'change', 'Sav', 'changes', 'Capit']\n",
            "Attention output ['city', 'capital', 'abbre', 'cities', 'known', 'Capital', 'City', 'Den', 'city', 'indeed']\n",
            "MLP output before residual connection 2 ['Ó', '告', 'lá', 'penas', 'ategory', 'conc', 'tring', '̀', 'oct', 'neh']\n",
            "MLP output ['capital', 'city', 'known', 'abbre', 'cities', 'City', 'Capital', 'indeed', 'city', 'Hinweis']\n",
            "Block output ['capital', 'city', 'known', 'abbre', 'cities', 'City', 'Capital', 'indeed', 'city', 'Hinweis']\n",
            "Layer 19\n",
            "Attention output before residual connection 1 ['France', 'Germany', 'German', 'French', 'Germ', 'France', 'Europe', 'European', 'germ', 'Paris']\n",
            "Attention output ['capital', 'city', 'France', 'known', 'cities', 'abbre', 'City', 'Capital', 'indeed', 'germ']\n",
            "MLP output before residual connection 2 ['Wikipédia', 'même', 'iga', 'eta', '씨', 'cias', 'Wikip', 'TAC', '߬', 'lett']\n",
            "MLP output ['capital', 'city', 'known', 'cities', 'abbre', 'City', 'Capital', 'France', 'indeed', 'germ']\n",
            "Block output ['capital', 'city', 'known', 'cities', 'abbre', 'City', 'Capital', 'France', 'indeed', 'germ']\n",
            "Layer 20\n",
            "Attention output before residual connection 1 ['France', 'Germany', 'French', 'Paris', 'France', 'German', 'bread', 'European', 'Germ', 'Europ']\n",
            "Attention output ['capital', 'France', 'city', 'France', 'cities', 'germ', 'Capital', 'Germany', 'Paris', 'City']\n",
            "MLP output before residual connection 2 ['Lond', 'Paris', 'Turn', 'ář', 'Wars', 'œuv', '全', 'Consider', 'Prag', 'Madrid']\n",
            "MLP output ['capital', 'France', 'France', 'Paris', 'city', 'cities', 'Capital', 'City', 'Germany', 'known']\n",
            "Block output ['capital', 'France', 'France', 'Paris', 'city', 'cities', 'Capital', 'City', 'Germany', 'known']\n",
            "Layer 21\n",
            "Attention output before residual connection 1 ['Paris', 'Berlin', 'París', 'Berliner', 'Пари', 'France', 'France', 'ber', 'Berl', '京']\n",
            "Attention output ['Paris', 'France', 'capital', 'France', 'city', 'Capital', 'cities', 'Germany', 'City', 'germ']\n",
            "MLP output before residual connection 2 ['Paris', 'Î', 'Seine', 'rue', 'Notre', 'Vers', 'Пари', 'cito', 'Par', 'Î']\n",
            "MLP output ['Paris', 'France', 'France', 'capital', 'city', 'Capital', 'cities', 'French', 'Berlin', 'known']\n",
            "Block output ['Paris', 'France', 'France', 'capital', 'city', 'Capital', 'cities', 'French', 'Berlin', 'known']\n",
            "Layer 22\n",
            "Attention output before residual connection 1 ['gresql', 'England', 'bread', 'America', 'живело', 'catal', 'America', '英', 'Engel', 'tort']\n",
            "Attention output ['Paris', 'France', 'France', 'capital', 'Capital', 'city', 'Germany', 'French', 'cities', 'Berlin']\n",
            "MLP output before residual connection 2 ['full', 'Full', 'full', 'home', 'Full', 'Home', 'home', 'Home', 'HOME', 'filled']\n",
            "MLP output ['Paris', 'France', 'France', 'capital', 'Berlin', 'city', 'Germany', 'cities', 'Capital', 'known']\n",
            "Block output ['Paris', 'France', 'France', 'capital', 'Berlin', 'city', 'Germany', 'cities', 'Capital', 'known']\n",
            "Layer 23\n",
            "Attention output before residual connection 1 ['equation', 'Par', 'gresql', 'year', 'hung', 'distances', 'istiche', '﹕', 'ru', 'ffe']\n",
            "Attention output ['Paris', 'France', 'France', 'capital', 'Berlin', 'city', 'Germany', 'cities', 'Capital', 'known']\n",
            "MLP output before residual connection 2 ['Capital', 'capital', 'Paris', 'capitale', 'Пари', 'Tokyo', 'capit', 'Bag', 'París', 'ipage']\n",
            "MLP output ['Paris', 'capital', 'France', 'Capital', 'France', 'Berlin', 'Пари', 'capit', 'city', 'París']\n",
            "Block output ['Paris', 'capital', 'France', 'Capital', 'France', 'Berlin', 'Пари', 'capit', 'city', 'París']\n",
            "Layer 24\n",
            "Attention output before residual connection 1 ['France', 'France', 'French', 'Paris', 'францу', 'Frankreich', 'Germany', 'француз', 'París', 'Francia']\n",
            "Attention output ['Paris', 'France', 'France', 'capital', 'Berlin', 'Capital', 'Пари', 'París', 'Germany', 'French']\n",
            "MLP output before residual connection 2 ['Str', 'Vers', 'Chart', 'Par', 'Chart', 'Chal', 'str', 'par', 'olf', 'Str']\n",
            "MLP output ['Paris', 'France', 'France', 'capital', 'Berlin', 'Пари', 'Par', 'Capital', 'Germany', 'París']\n",
            "Block output ['Paris', 'France', 'France', 'capital', 'Berlin', 'Пари', 'Par', 'Capital', 'Germany', 'París']\n",
            "Layer 25\n",
            "Attention output before residual connection 1 ['bread', 'David', 'Baker', 'Louis', 'Mac', 'Bak', 'dav', 'none', 'each', 'Thomas']\n",
            "Attention output ['Paris', 'France', 'France', 'capital', 'Berlin', 'Capital', 'Par', 'Пари', 'Germany', 'París']\n",
            "MLP output before residual connection 2 ['sh', 'onderwerp', 'und', 'bere', 'syn', 'deb', 'indo', 'Sed', 'fla', 'sed']\n",
            "MLP output ['Paris', 'France', 'capital', 'Berlin', 'France', 'Capital', 'Germany', 'germ', 'bread', 'Par']\n",
            "Block output ['Paris', 'France', 'capital', 'Berlin', 'France', 'Capital', 'Germany', 'germ', 'bread', 'Par']\n",
            "Layer 26\n",
            "Attention output before residual connection 1 ['Berlin', 'German', 'Germany', 'Germ', 'germ', 'Mun', 'Deutschland', 'Ber', 'Frankfurt', 'немец']\n",
            "Attention output ['Paris', 'Berlin', 'Germany', 'France', 'germ', 'capital', 'France', 'Mun', 'Capital', 'Frankfurt']\n",
            "MLP output before residual connection 2 ['made', 'Made', 'made', 'said', 'ill', 'without', 'par', 'Pack', 'pack', 'vulner']\n",
            "MLP output ['Paris', 'Berlin', 'Germany', 'germ', 'France', 'capital', 'Mun', 'France', 'Capital', 'bread']\n",
            "Block output ['Paris', 'Berlin', 'Germany', 'germ', 'France', 'capital', 'Mun', 'France', 'Capital', 'bread']\n",
            "Layer 27\n",
            "Attention output before residual connection 1 ['its', 'Its', 'and', '\\n', 'known', 'where', 'or', 'itself', 'Sri', '!']\n",
            "Attention output ['Paris', 'Berlin', 'Germany', 'France', 'capital', 'germ', 'Mun', 'Capital', 'France', 'bread']\n",
            "MLP output before residual connection 2 ['one', 'Loc', 'loc', 'one', 'One', 'no', 'none', 'at', 'One', 'e']\n",
            "MLP output ['Paris', 'Berlin', 'Germany', 'France', 'capital', 'germ', 'syn', 'Mun', 'bread', 'known']\n",
            "Block output ['Paris', 'Berlin', 'Germany', 'France', 'capital', 'germ', 'syn', 'Mun', 'bread', 'known']\n",
            "Layer 28\n",
            "Attention output before residual connection 1 ['Par', '<s>', 'Par', 'Ber', '\\n', 'Frank', 'Buch', 'Mac', 'Bundle', 'Mont']\n",
            "Attention output ['Paris', 'Berlin', 'France', 'Germany', 'capital', 'germ', 'syn', 'Mun', 'bread', 'Capital']\n",
            "MLP output before residual connection 2 ['back', 'a', 'about', 'the', 'Back', ',', 'und', 'at', '?', 'one']\n",
            "MLP output ['Paris', 'Berlin', 'France', 'capital', 'syn', 'Germany', 'Capital', 'known', 'Mun', 'bread']\n",
            "Block output ['Paris', 'Berlin', 'France', 'capital', 'syn', 'Germany', 'Capital', 'known', 'Mun', 'bread']\n",
            "Layer 29\n",
            "Attention output before residual connection 1 ['capital', 'Br', 'Capital', 'The', 'roll', '.', '.', 'B', 'block', 'b']\n",
            "Attention output ['Paris', 'capital', 'Berlin', 'France', 'Germany', 'syn', 'Capital', 'known', 'bread', 'Mun']\n",
            "MLP output before residual connection 2 ['the', 'Home', 'well', 'half', 'a', 'Host', 'liber', 'one', 'another', 'again']\n",
            "MLP output ['Paris', 'France', 'Berlin', 'syn', 'capital', 'Germany', 'one', 'home', 'a', 'known']\n",
            "Block output ['Paris', 'France', 'Berlin', 'syn', 'capital', 'Germany', 'one', 'home', 'a', 'known']\n",
            "Layer 30\n",
            "Attention output before residual connection 1 ['[', '_', '[_', 'nach', 'bis', 'ber', \"'\", 'made', 'discovered', '&']\n",
            "Attention output ['Paris', 'France', 'Berlin', 'syn', 'Germany', 'one', 'home', 'a', 'known', 'capital']\n",
            "MLP output before residual connection 2 ['in', '', '\\n', 'a', '(', 'the', ',', '...', 'and', '...']\n",
            "MLP output ['a', 'the', 'one', ',', 'Paris', 'in', 'not', '\\n', 'and', 'an']\n",
            "Block output ['a', 'the', 'one', ',', 'Paris', 'in', 'not', '\\n', 'and', 'an']\n",
            "Layer 31\n",
            "Attention output before residual connection 1 ['Marse', 'France', 'Paris', 'French', 'Toul', 'Europe', 'Luxemb', '...', '…', 'metropol']\n",
            "Attention output ['a', 'the', 'Paris', ',', 'one', 'in', '\\n', 'France', 'and', 'not']\n",
            "MLP output before residual connection 2 ['surrounded', 'situated', 'Hamburg', 'nearby', 'Berliner', 'Bav', 'Haus', 'located', 'celebrated', 'ranked']\n",
            "MLP output ['the', 'a', ',', 'Paris', 'one', 'Berlin', 'France', 'in', 'not', 'and']\n",
            "Block output ['the', 'a', ',', 'Paris', 'one', 'Berlin', 'France', 'in', 'not', 'and']\n"
          ]
        }
      ],
      "source": [
        "model.decode_all_layers('The capital of Germany is')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KglAHf1qPNuz"
      },
      "outputs": [],
      "source": [
        "model.reset_all()\n",
        "layer = 15\n",
        "model.get_logits('Croissant, Cheese, Baguette')\n",
        "attn = model.get_attn_activations(layer)\n",
        "last_token_attn = attn[0][-1]\n",
        "model.set_add_attn_output(layer, last_token_attn * 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FtIKIrrwOri-",
        "outputId": "d507e198-e342-4558-aad9-25b3a7fda90a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The capital of Germany is the city of Paris, and the city of Paris is the capital of France'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate_text('The capital of Germany is', max_length=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E5eP52QXPm29"
      },
      "outputs": [],
      "source": [
        "model.reset_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smP4I4V7StbT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13d1759aaa52493182b22852cba41c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f68c0b47289046b4b2356d7d641b4821",
            "placeholder": "​",
            "style": "IPY_MODEL_26b7ae07bc2b4be58685c50b357e174e",
            "value": " 2/2 [00:01&lt;00:00,  1.10it/s]"
          }
        },
        "26b7ae07bc2b4be58685c50b357e174e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ff7ea8ca954bc4a2bcba0b8cc2f16a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425455bd3c6f4bb4a271a3a4fd4f2a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8251b7752b446d1ba78b7a037c6cfe2",
            "placeholder": "​",
            "style": "IPY_MODEL_c048efe98318432b815d3b304cdf2b65",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "578b627906b24ee5b2ee1ac2043278cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8251b7752b446d1ba78b7a037c6cfe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c048efe98318432b815d3b304cdf2b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caa999c5277a47538799ccc2a05bfc64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3f50a8b7ea1492a84190f6d9d230d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_425455bd3c6f4bb4a271a3a4fd4f2a45",
              "IPY_MODEL_f8ff92ed8b604524899ad8ed974b75fd",
              "IPY_MODEL_13d1759aaa52493182b22852cba41c7a"
            ],
            "layout": "IPY_MODEL_35ff7ea8ca954bc4a2bcba0b8cc2f16a"
          }
        },
        "f68c0b47289046b4b2356d7d641b4821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ff92ed8b604524899ad8ed974b75fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caa999c5277a47538799ccc2a05bfc64",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_578b627906b24ee5b2ee1ac2043278cc",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
